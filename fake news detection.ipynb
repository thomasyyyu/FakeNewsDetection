{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class to read the dataset\r\n",
    "class Dataset():\r\n",
    "  def __init__(self, train_stance, test_stance, train_body, test_body):\r\n",
    "    self.train_stance = train_stance\r\n",
    "    self.test_stance = test_stance\r\n",
    "    self.train_body = train_body\r\n",
    "    self.test_body = test_body\r\n",
    "\r\n",
    "    print(\"Dataset length:\")\r\n",
    "\r\n",
    "    self.train_stances = self.read_stance(self.train_stance)\r\n",
    "    self.test_stances = self.read_stance(self.test_stance)\r\n",
    "    self.train_bodies = self.read_body(self.train_body)\r\n",
    "    self.test_bodies = self.read_body(self.test_body)\r\n",
    "\r\n",
    "    print(\"Total train stances: \" + str(len(self.train_stances)))\r\n",
    "    print(\"Total test stances: \" + str(len(self.test_stances)))\r\n",
    "    print(\"Total train bodies: \" + str(len(self.train_bodies)))\r\n",
    "    print(\"Total test bodies: \" + str(len(self.test_bodies)))\r\n",
    "\r\n",
    "  def read_stance(self, path):\r\n",
    "    rows = []\r\n",
    "    with open(path, encoding='utf-8', errors='ignore') as csvfile:\r\n",
    "      r = csv.DictReader(csvfile)\r\n",
    "      for row in r:\r\n",
    "        rows.append([row['Body ID'], row['Headline'], row['Stance']])\r\n",
    "    return rows\r\n",
    "\r\n",
    "  def read_body(self, path):\r\n",
    "    rows = []\r\n",
    "    #with open(path, encoding='utf-8') as csvfile:\r\n",
    "    with open(path, encoding=\"utf8\", errors='ignore') as csvfile:\r\n",
    "      r = csv.DictReader(csvfile)\r\n",
    "      for row in r:\r\n",
    "        rows.append([row['Body ID'], row['articleBody']])\r\n",
    "        #rows[row['Body ID']] = row['articleBody']\r\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:\n",
      "Total train stances: 49972\n",
      "Total test stances: 25413\n",
      "Total train bodies: 1683\n",
      "Total test bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# load the data\r\n",
    "data = Dataset('./fnc1/train_stances.csv', './fnc1/competition_test_stances.csv', './fnc1/train_bodies.csv', './fnc1/competition_test_bodies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['2008',\n 'Ferguson riots: Pregnant woman loses eye after cops fire BEAN BAG round through car window',\n 'unrelated']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_stances[0]\r\n",
    "\r\n",
    "data.test_stances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['1',\n 'Al-Sisi has denied Israeli reports stating that he offered to extend the Gaza Strip.']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_bodies[0]\r\n",
    "data.test_bodies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process function (lowercase, stopwords, lemmatization)\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(processed_dataset):\n",
    "  for w in processed_dataset:\n",
    "    words = []\n",
    "    w[1] = word_tokenize(w[1])\n",
    "    for token in w[1]:\n",
    "      lower = token.lower()\n",
    "      if lower not in stopwords and lower.isalpha():\n",
    "        word = porter.stem(lower)\n",
    "        words.append(word)\n",
    "    w[1] = words\n",
    "  return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call pre-process functions for train and test\r\n",
    "train_headline = preprocess(data.train_stances)\r\n",
    "test_headline = preprocess(data.test_stances)\r\n",
    "train_content = preprocess(data.train_bodies)\r\n",
    "test_content = preprocess(data.test_bodies)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', ['soldier', 'shot', 'parliament', 'lock', 'gunfir', 'erupt', 'war', 'memori'], 'unrelated']\n",
      "['2008', ['ferguson', 'riot', 'pregnant', 'woman', 'lose', 'eye', 'cop', 'fire', 'bean', 'bag', 'round', 'car', 'window'], 'unrelated']\n",
      "['0', ['small', 'meteorit', 'crash', 'wood', 'area', 'nicaragua', 'capit', 'managua', 'overnight', 'govern', 'said', 'sunday', 'resid', 'report', 'hear', 'mysteri', 'boom', 'left', 'deep', 'crater', 'near', 'citi', 'airport', 'associ', 'press', 'report', 'govern', 'spokeswoman', 'rosario', 'murillo', 'said', 'committe', 'form', 'govern', 'studi', 'event', 'determin', 'rel', 'small', 'meteorit', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'asteroid', 'rc', 'measur', 'feet', 'diamet', 'skim', 'earth', 'weekend', 'abc', 'news', 'report', 'murillo', 'said', 'nicaragua', 'ask', 'intern', 'expert', 'help', 'local', 'scientist', 'understand', 'happen', 'crater', 'left', 'meteorit', 'radiu', 'feet', 'depth', 'feet', 'said', 'humberto', 'saballo', 'volcanologist', 'nicaraguan', 'institut', 'territori', 'studi', 'committe', 'said', 'still', 'clear', 'meteorit', 'disintegr', 'buri', 'humberto', 'garcia', 'astronomi', 'center', 'nation', 'autonom', 'univers', 'nicaragua', 'said', 'meteorit', 'could', 'relat', 'asteroid', 'forecast', 'pass', 'planet', 'saturday', 'night', 'studi', 'could', 'ice', 'rock', 'said', 'wilfri', 'strauch', 'advis', 'institut', 'territori', 'studi', 'said', 'strang', 'one', 'report', 'streak', 'light', 'ask', 'anyon', 'photo', 'someth', 'local', 'resid', 'report', 'hear', 'loud', 'boom', 'saturday', 'night', 'said', 'see', 'anyth', 'strang', 'sky', 'sit', 'porch', 'saw', 'noth', 'sudden', 'heard', 'larg', 'blast', 'thought', 'bomb', 'felt', 'expans', 'wave', 'jorg', 'santamaria', 'told', 'associ', 'press', 'site', 'crater', 'near', 'managua', 'intern', 'airport', 'air', 'forc', 'base', 'journalist', 'state', 'media', 'allow', 'visit']]\n",
      "['1', ['deni', 'isra', 'report', 'state', 'offer', 'extend', 'gaza', 'strip']]\n"
     ]
    }
   ],
   "source": [
    "print(train_headline[0])\r\n",
    "print(test_headline[0])\r\n",
    "print(train_content[0])\r\n",
    "print(test_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all sentences into one collection for word2vec training.\r\n",
    "# It takes word tokenized sentences, which looks like [\"hello\", \"world\", ... ]\r\n",
    "\r\n",
    "sent_collection = []\r\n",
    "\r\n",
    "def sent_list(s_list, t_list):\r\n",
    "  for sent in s_list:\r\n",
    "    t_list.append(sent[1])\r\n",
    "  return\r\n",
    "\r\n",
    "sent_list(train_headline, sent_collection)\r\n",
    "sent_list(test_headline, sent_collection)\r\n",
    "sent_list(train_content, sent_collection)\r\n",
    "sent_list(test_content, sent_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soldier', 'shot', 'parliament', 'lock', 'gunfir', 'erupt', 'war', 'memori']\n"
     ]
    }
   ],
   "source": [
    "print(sent_collection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084\n"
     ]
    }
   ],
   "source": [
    "# to see how many unique words in the collection\r\n",
    "bag = []\r\n",
    "for s in sent_collection:\r\n",
    "  for w in s:\r\n",
    "    bag.append(w)\r\n",
    "\r\n",
    "p = set(bag)\r\n",
    "print(len(p))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the word2vec with customized words because the stemming makes some words not recognizable, such as \"polic\" and \"strang\"\r\n",
    "# details  https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\r\n",
    "\r\n",
    "#import multiprocessing\r\n",
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "#cores = multiprocessing.cpu_count()\r\n",
    "w2v_model = Word2Vec(min_count=1,\r\n",
    "                    window=2,\r\n",
    "                    size=100,\r\n",
    "                    sample=6e-5, \r\n",
    "                    alpha=0.03, \r\n",
    "                    min_alpha=0.0007, \r\n",
    "                    negative=20)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sent_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(17339019, 33276930)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sent_collection, total_examples=w2v_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes the memory more efficient since we do not plan tot train any further\r\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "18084"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.03170666,  0.05994361, -0.01594953, -0.01158441, -0.2553048 ,\n        0.15896626, -0.05707419,  0.06622464,  0.1167105 ,  0.02071714,\n        0.01473159,  0.08760121,  0.02629115,  0.06877466,  0.14730984,\n        0.00831118, -0.00957257,  0.0364738 ,  0.01016295, -0.00858984,\n        0.01968961, -0.12182599, -0.00251548, -0.10242035,  0.01302129,\n       -0.03282655, -0.11320949,  0.11239958,  0.10937969, -0.10672441,\n       -0.00179643, -0.04826476,  0.01923433,  0.18598905,  0.15126215,\n       -0.17327411, -0.02258573, -0.0134278 ,  0.0591633 ,  0.15780906,\n        0.09686403, -0.03460663, -0.20300788, -0.06884312,  0.10651762,\n       -0.02109622,  0.09848445,  0.09285858, -0.01213906, -0.0651632 ,\n        0.10628242, -0.02083327, -0.03462162, -0.08759736,  0.21147734,\n        0.04124995,  0.00487584,  0.03875668,  0.0316501 ,  0.00573582,\n        0.13343246,  0.10616208,  0.11232797, -0.03080473, -0.1765491 ,\n        0.01725558, -0.21733336,  0.0200403 ,  0.1301482 , -0.19204529,\n       -0.0253341 ,  0.03048412, -0.157898  , -0.10348459, -0.11907003,\n       -0.1147286 , -0.05432421, -0.07850005,  0.13270453, -0.04214988,\n       -0.04537453, -0.01052979,  0.01296469, -0.08103745,  0.21435371,\n        0.02875281, -0.09801527, -0.18954329,  0.1973032 ,  0.13804442,\n       -0.02303872, -0.09735751, -0.02806089, -0.00513703, -0.09414748,\n       -0.07477094,  0.04246349, -0.05472517, -0.08868736, -0.15728256],\n      dtype=float32)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector('polic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.053465083"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('polic', 'strang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine headline and body\r\n",
    "import nltk\r\n",
    "def comb_list (stance, body, target):\r\n",
    "  for i in body:\r\n",
    "    for j in stance:\r\n",
    "      if j[0] == i[0]:\r\n",
    "        i[1] = nltk.FreqDist(i[1])\r\n",
    "        target.append([j[0], j[1], i[1], j[2]])\r\n",
    "  return\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "comb_list(train_headline, train_content, train_set)\n",
    "comb_list(test_headline, test_content, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25413\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5c0372ed38b372118c24adb00d45654d76c8d10261533c5724e3f5fc1d75489a"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}